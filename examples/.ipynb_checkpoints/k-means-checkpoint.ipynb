{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from sklearn.datasets.samples_generator import make_blobs\n",
    "from pyspark import SparkContext\n",
    "from pyspark.ml.clustering import KMeans\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.sql import SQLContext\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print (pyspark.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples=1000\n",
    "n_features=3\n",
    "X, y = make_blobs(n_samples=n_samples, centers=5, n_features=n_features, random_state=1)\n",
    "\n",
    "pddf = pd.DataFrame(X, columns=['x', 'y', 'z'])\n",
    "pddf['id'] = 'r' + pddf.index.astype(str)\n",
    "\n",
    "cols = list(pddf.columns)\n",
    "\n",
    "cols.insert(0, cols.pop(cols.index('id')))\n",
    "\n",
    "pddf = pddf.loc[:, cols]\n",
    "pddf.head(5)\n",
    "pddf.to_csv('input.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "D3plot = plt.figure(figsize=(9,7)).gca(projection='3d')\n",
    "D3plot.scatter(X[:,0], X[:,1], X[:,2], c=y)\n",
    "D3plot.set_xlabel('x')\n",
    "D3plot.set_ylabel('y')\n",
    "D3plot.set_zlabel('z')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sc = SparkContext(appName=\"k-means\")\n",
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FEATURES_COL = ['x', 'y', 'z']\n",
    "path = 'input.csv'\n",
    "df = sqlContext.read.csv(path, header=True) # requires spark 2.0\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = sc.textFile(path)\n",
    "data = lines.map(lambda line: line.split(\",\"))\n",
    "data.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
